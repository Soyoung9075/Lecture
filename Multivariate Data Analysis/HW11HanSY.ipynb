{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write a Python code to calculate the linear discriminant function for several classes. Your code should be able to predict the Y class based on the input ùë•! values. (Page 31 in the lecture note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LDA_pred(dfX, dfy, prior, new_x):\n",
    "    \n",
    "    n_group = dfy.nunique()\n",
    "    cov_list = []\n",
    "    mean_list = []\n",
    "    ldf_list = []\n",
    "    n_list = []\n",
    "\n",
    "    for i in range(n_group):\n",
    "        X = dfX.iloc[dfy[dfy == i+1].index, :]\n",
    "        n_list.append(len(X))\n",
    "        mean_list.append(X.mean())\n",
    "        cov_list.append(X.cov())\n",
    "\n",
    "    def pooled_cov(n_list, cov_list, n_group):\n",
    "        result = 0\n",
    "        for x, y in zip(n_list, cov_list):\n",
    "            product = (x-1)*y\n",
    "            result += product\n",
    "\n",
    "        pooled_cov = (1 / (np.sum(n_list)- n_group)) * result\n",
    "        return pooled_cov\n",
    "\n",
    "    s_p = pooled_cov(n_list, cov_list, n_group)\n",
    "\n",
    "    coeff=[]\n",
    "    intercept=[]\n",
    "    d=[]\n",
    "    for j in range(n_group):\n",
    "        coeff.append(mean_list[j] @ np.linalg.inv(s_p))\n",
    "        intercept.append(-1/2 * (mean_list[j] @ np.linalg.inv(s_p) @ mean_list[j].T))\n",
    "        ldf = (mean_list[j] @ np.linalg.inv(s_p) @ np.array(new_x))- 1/2 * (mean_list[j] @ np.linalg.inv(s_p) @ mean_list[j].T) + np.log(prior[j])\n",
    "        d.append(ldf)\n",
    "    return_class = d.index(max(d))+1\n",
    "    \n",
    "    return intercept, coeff, d, max(d), return_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write a Python code to calculate the quadratic discriminant function for several classes. Your code should be able to predict the Y class based on the input ùë•! values. (Page 35 in the lecture note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QDA_pred(dfX, dfy, prior, new_x):\n",
    "    \n",
    "    n_group = dfy.nunique()\n",
    "    cov_list = []\n",
    "    mean_list = []\n",
    "    ldf_list = []\n",
    "    n_list = []\n",
    "\n",
    "    for i in range(n_group):\n",
    "        X = dfX.iloc[dfy[dfy == i+1].index, :]\n",
    "        n_list.append(len(X))\n",
    "        mean_list.append(X.mean())\n",
    "        cov_list.append(X.cov())\n",
    "\n",
    "    d=[]\n",
    "    for j in range(n_group):\n",
    "        ldf = -1/2*np.log(np.linalg.det(cov_list[j])) - 1/2*(new_x - mean_list[j])@np.linalg.inv(cov_list[j])@(new_x - mean_list[j]).T + np.log(prior[j])\n",
    "        d.append(ldf)\n",
    "    return_class = d.index(max(d))+1\n",
    "    \n",
    "    return d, max(d), return_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a Python code to perform the 'leave-one-out' method to calculate the accuracy of the LDA & QDA model you wrote in #1 & #2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOO(dfX, dfy, prior, model):\n",
    "    pred_y = []\n",
    "    if model == 'LDA':\n",
    "        for i in range(len(dfX)):\n",
    "            newdata = dfX.iloc[i,:].tolist()\n",
    "            X_train = dfX.drop(dfX.index[i]).reset_index().drop(['index'], axis = 1)\n",
    "            y_train = dfy.drop(dfy.index[i]).reset_index().drop(['index'], axis = 1)[4]\n",
    "            intercept, coeff, d_list, max_d, return_class = LDA_pred(X_train, y_train, prior, newdata)\n",
    "            pred_y.append(return_class)\n",
    "    else:\n",
    "        for i in range(len(dfX)):\n",
    "            newdata = dfX.iloc[i,:].tolist()\n",
    "            X_train = dfX.drop(dfX.index[i]).reset_index().drop(['index'], axis = 1)\n",
    "            y_train = dfy.drop(dfy.index[i]).reset_index().drop(['index'], axis = 1)[4]\n",
    "            d_list, max_d, return_class = QDA_pred(X_train, y_train, prior, newdata)\n",
    "            pred_y.append(return_class)\n",
    "    result = pd.DataFrame({'original_y' : dfy, 'pred_y' : pred_y})\n",
    "    F_result = result.loc[result['original_y']!=result['pred_y']]\n",
    "    A_result = result.loc[result['original_y']==result['pred_y']]\n",
    "    Accuracy_Rate = len(A_result) / len(result)\n",
    "    error_rate = len(F_result) / len(result)\n",
    "    return Accuracy_Rate, error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Using Fisher's Iris data (Table 11.5), answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1    2    3  4\n",
       "0  5.1  3.5  1.4  0.2  1\n",
       "1  4.9  3.0  1.4  0.2  1\n",
       "2  4.7  3.2  1.3  0.2  1\n",
       "3  4.6  3.1  1.5  0.2  1\n",
       "4  5.0  3.6  1.4  0.2  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = pd.read_csv('iris.dat', header = None, delim_whitespace=True)\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Is the assumption of a common covariance matrix reasonable in this case? (Use Python's 'statsmodels.stats.multivariate' module for this question)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Test statistic: 140.94304992349774  , Pr > ChiSq: 3.352034178317213e-20\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats import multivariate as mv\n",
    "group1 = iris.loc[iris[4]==1, 0:3]\n",
    "group2 = iris.loc[iris[4]==2, 0:3]\n",
    "group3 = iris.loc[iris[4]==3, 0:3]\n",
    "\n",
    "cov1 = group1.cov() ; cov2 = group2.cov() ; cov3 = group3.cov()\n",
    "\n",
    "test = mv.test_cov_oneway([cov1,cov2,cov3],[len(group1),len(group2),len(group3)])\n",
    "print(\"Chi-Square Test statistic:\",test.statistic_chi2, \" , Pr > ChiSq:\",test.pvalue_chi2)\n",
    "\n",
    "# Since p-value is low, Do reject H0 => QDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Assuming that the populations are multivariate normal, calculate the quadratic discriminant scores with equal prior and equal misclassification cost. Classify the new observation ùë•! = [5.0, 3.5, 1.75, 0.21]‚Ä≤ into one of populations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadratic discriminant score : [3.4909763650152277, -47.30137268292963, -74.70869273851915]\n",
      "Predicted class for new observation : 1\n"
     ]
    }
   ],
   "source": [
    "prior = [1/3, 1/3, 1/3]\n",
    "new_x = [5.0, 3.5, 1.75, 0.21]\n",
    "dfX = iris.iloc[:, 0:4]\n",
    "dfy = iris[4]\n",
    "\n",
    "d_list, max_d, pred_class = QDA_pred(dfX, dfy, prior, new_x)\n",
    "\n",
    "print(\"Quadratic discriminant score : {}\".format(d_list))\n",
    "print(\"Predicted class for new observation : {}\".format(pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Assuming equal covariance matrices and multivariate normal populations, calculate the linear discriminant function using your code in #1 above and compare its coefficients with those of Python's 'sklearn.discriminant_analysis' module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept : [-85.20985768500591, -71.75399511197409, -103.26970769778173]\n",
      "Coefficeint : \n",
      "[ 23.54416672  23.5878705  -16.43063902 -17.39841078]\n",
      "[15.69820908  7.07250984  5.21145093  6.4342292 ]\n",
      "[12.44584899  3.68527961 12.76654497 21.07911301]\n",
      "LDF score : [81.56262582121121, 40.86344967695457, -2.472528938799388]\n",
      "Predicted class for new observation : 1\n"
     ]
    }
   ],
   "source": [
    "prior = [1/3, 1/3, 1/3]\n",
    "new_x = [5.0, 3.5, 1.75, 0.21]\n",
    "dfX = iris.iloc[:, 0:4]\n",
    "dfy = iris[4]\n",
    "\n",
    "intercept, coeff, d_list, max_d, pred_class = LDA_pred(dfX, dfy, prior, new_x)\n",
    "print(\"Intercept : {}\".format(intercept))\n",
    "print(\"Coefficeint : \\n{}\\n{}\\n{}\".format(coeff[0], coeff[1], coeff[2]))\n",
    "print(\"LDF score : {}\".format(d_list))\n",
    "print(\"Predicted class for new observation : {}\".format(pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearDiscriminantAnalysis(priors=[0.3333333333333333, 0.3333333333333333,\n",
       "                                   0.3333333333333333],\n",
       "                           solver=&#x27;eigen&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearDiscriminantAnalysis</label><div class=\"sk-toggleable__content\"><pre>LinearDiscriminantAnalysis(priors=[0.3333333333333333, 0.3333333333333333,\n",
       "                                   0.3333333333333333],\n",
       "                           solver=&#x27;eigen&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearDiscriminantAnalysis(priors=[0.3333333333333333, 0.3333333333333333,\n",
       "                                   0.3333333333333333],\n",
       "                           solver='eigen')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "X = iris.iloc[:,0:4]\n",
    "Y = iris.iloc[:,4]\n",
    "lda = LDA(priors = prior, solver = 'eigen')\n",
    "lda.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept : [ -88.04744666  -74.31697465 -106.47586504]\n",
      "Coefficeint : \n",
      "[ 24.02465992  24.06925561 -16.76595819 -17.75348039]\n",
      "[16.01858069  7.21684677  5.31780708  6.56554   ]\n",
      "[12.69984591  3.7604894  13.02708671 21.50929899]\n"
     ]
    }
   ],
   "source": [
    "print(\"Intercept : {}\".format(lda.intercept_))\n",
    "print(\"Coefficeint : \\n{}\\n{}\\n{}\".format(lda.coef_[0], lda.coef_[1], lda.coef_[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Calculate and compare the APER and the leave-one-out error rates for linear discriminant analysis (LDA) and quadratic discriminant analysis (QDA) using your code in #1,2,3. (Assume equal prior and equal misclassification cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def APER(dfX, dfy, prior, model):\n",
    "    pred_y = []\n",
    "    if model == 'LDA':\n",
    "        for i in range(len(dfX)):\n",
    "            newdata = dfX.iloc[i,:].tolist()\n",
    "            intercept, coeff, d_list, max_d, return_class = LDA_pred(dfX, dfy, prior, newdata)\n",
    "            pred_y.append(return_class)\n",
    "    else:\n",
    "        for i in range(len(dfX)):\n",
    "            newdata = dfX.iloc[i,:].tolist()\n",
    "            intercept, coeff, d_list, max_d, return_class = LDA_pred(dfX, dfy, prior, newdata)\n",
    "            pred_y.append(return_class)\n",
    "    result = pd.DataFrame({'original_y' : dfy, 'pred_y' : pred_y})\n",
    "    F_result = result.loc[result['original_y']!=result['pred_y']]\n",
    "    A_result = result.loc[result['original_y']==result['pred_y']]\n",
    "    error_rate = len(F_result) / len(result)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APER for LDA : 0.02\n",
      "Leave-one-out error rate for LDA : 0.02\n"
     ]
    }
   ],
   "source": [
    "print(\"APER for LDA : {}\".format(APER(dfX, dfy, prior, 'LDA')))\n",
    "print(\"Leave-one-out error rate for LDA : {}\".format(LOO(dfX, dfy, prior, 'LDA')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APER for QDA : 0.02\n",
      "Leave-one-out error rate for QDA : 0.0267\n"
     ]
    }
   ],
   "source": [
    "print(\"APER for QDA : {}\".format(APER(dfX, dfy, prior, 'QDA')))\n",
    "print(\"Leave-one-out error rate for QDA : {}\".format(round(LOO(dfX, dfy, prior, 'QDA')[1],4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
